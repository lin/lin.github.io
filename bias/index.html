<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Bias Prone Machine - Yingkui Lin</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Bias Prone Machine"><meta itemprop=description content="Human minds are prone to holding on to false arguments and wrong ideas for several interacting reasons:
Confirmation bias, We tend to seek out or give extra weight to information that fits our existing beliefs, and ignore or downplay anything that contradicts them. Availability heuristic, Events or arguments that are more vivid or recently encountered feel more plausible than they objectively are. Dunning–Kruger effect, People with limited knowledge often overestimate their own competence, so they cling to simplistic but incorrect views. Strong priors, In the Bayesian brain framework, your “prior” beliefs act as predictions. When these priors are very confident, any new sensory evidence (prediction error) is down‐weighted and you won’t update your belief much. Precision weighting, The brain assigns a precision (inverse variance) to both priors and sensory data. If you overestimate the precision of your priors, you’ll ignore disconfirming evidence and reinforce a false idea. Identity‑protective cognition, Beliefs that tie into your group identity (political, religious, cultural) feel threatening if challenged, so you resist changing them even in the face of facts. Authority and conformity, If a respected leader or majority holds a wrong idea, you may adopt it to fit in or out of deference, rather than examine it critically. Backfire effect, Attempts to correct a false belief can paradoxically reinforce it, because disconfirming evidence feels like a personal attack. Sunk‐cost fallacy, Once you’ve invested time, emotion or reputation in an idea, admitting you were wrong feels like losing that investment. Effort avoidance, Deeply evaluating every claim takes time and mental energy. Under cognitive load or stress, we fall back on heuristics and old beliefs. Information overload, Faced with vast amounts of conflicting information online, it’s easier to latch onto a simple narrative—even if it’s false. Halo effect. You form a strong positive (or negative) impression of a person, group or brand based on one salient trait. In Bayesian terms, that gives you an overly precise prior $p(z)$ about anything they say or do. New evidence from that source is up‑weighted—prediction errors are down‑weighted—so you keep believing them even when they’re wrong. Cognitive dissonance, When new evidence $x$ conflicts sharply with a held belief (prior $μ_{\rm prior}$), it generates large prediction error. To minimize that error the brain can either update its belief or reinterpret/ignore the evidence. Often it “chooses” the latter, twisting the data to fit the prior rather than admit error. Strong priors (from repetition, emotion, expertise, halo) plus error‑minimizing drives (disfavoring belief change under dissonance) lock you into false ideas. Under stress, group polarization or fatigue, precision on priors skyrockets and the brain leans even harder on these biases, making belief revision very unlikely."><meta itemprop=datePublished content="2025-07-24T00:00:00+00:00"><meta itemprop=dateModified content="2025-07-24T00:00:00+00:00"><meta itemprop=wordCount content="440"><meta property="og:url" content="https://yingkui.com/bias/"><meta property="og:site_name" content="Yingkui Lin"><meta property="og:title" content="Bias Prone Machine"><meta property="og:description" content="Human minds are prone to holding on to false arguments and wrong ideas for several interacting reasons:
Confirmation bias, We tend to seek out or give extra weight to information that fits our existing beliefs, and ignore or downplay anything that contradicts them. Availability heuristic, Events or arguments that are more vivid or recently encountered feel more plausible than they objectively are. Dunning–Kruger effect, People with limited knowledge often overestimate their own competence, so they cling to simplistic but incorrect views. Strong priors, In the Bayesian brain framework, your “prior” beliefs act as predictions. When these priors are very confident, any new sensory evidence (prediction error) is down‐weighted and you won’t update your belief much. Precision weighting, The brain assigns a precision (inverse variance) to both priors and sensory data. If you overestimate the precision of your priors, you’ll ignore disconfirming evidence and reinforce a false idea. Identity‑protective cognition, Beliefs that tie into your group identity (political, religious, cultural) feel threatening if challenged, so you resist changing them even in the face of facts. Authority and conformity, If a respected leader or majority holds a wrong idea, you may adopt it to fit in or out of deference, rather than examine it critically. Backfire effect, Attempts to correct a false belief can paradoxically reinforce it, because disconfirming evidence feels like a personal attack. Sunk‐cost fallacy, Once you’ve invested time, emotion or reputation in an idea, admitting you were wrong feels like losing that investment. Effort avoidance, Deeply evaluating every claim takes time and mental energy. Under cognitive load or stress, we fall back on heuristics and old beliefs. Information overload, Faced with vast amounts of conflicting information online, it’s easier to latch onto a simple narrative—even if it’s false. Halo effect. You form a strong positive (or negative) impression of a person, group or brand based on one salient trait. In Bayesian terms, that gives you an overly precise prior $p(z)$ about anything they say or do. New evidence from that source is up‑weighted—prediction errors are down‑weighted—so you keep believing them even when they’re wrong. Cognitive dissonance, When new evidence $x$ conflicts sharply with a held belief (prior $μ_{\rm prior}$), it generates large prediction error. To minimize that error the brain can either update its belief or reinterpret/ignore the evidence. Often it “chooses” the latter, twisting the data to fit the prior rather than admit error. Strong priors (from repetition, emotion, expertise, halo) plus error‑minimizing drives (disfavoring belief change under dissonance) lock you into false ideas. Under stress, group polarization or fatigue, precision on priors skyrockets and the brain leans even harder on these biases, making belief revision very unlikely."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-24T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Bias Prone Machine"><meta name=twitter:description content="Human minds are prone to holding on to false arguments and wrong ideas for several interacting reasons:
Confirmation bias, We tend to seek out or give extra weight to information that fits our existing beliefs, and ignore or downplay anything that contradicts them. Availability heuristic, Events or arguments that are more vivid or recently encountered feel more plausible than they objectively are. Dunning–Kruger effect, People with limited knowledge often overestimate their own competence, so they cling to simplistic but incorrect views. Strong priors, In the Bayesian brain framework, your “prior” beliefs act as predictions. When these priors are very confident, any new sensory evidence (prediction error) is down‐weighted and you won’t update your belief much. Precision weighting, The brain assigns a precision (inverse variance) to both priors and sensory data. If you overestimate the precision of your priors, you’ll ignore disconfirming evidence and reinforce a false idea. Identity‑protective cognition, Beliefs that tie into your group identity (political, religious, cultural) feel threatening if challenged, so you resist changing them even in the face of facts. Authority and conformity, If a respected leader or majority holds a wrong idea, you may adopt it to fit in or out of deference, rather than examine it critically. Backfire effect, Attempts to correct a false belief can paradoxically reinforce it, because disconfirming evidence feels like a personal attack. Sunk‐cost fallacy, Once you’ve invested time, emotion or reputation in an idea, admitting you were wrong feels like losing that investment. Effort avoidance, Deeply evaluating every claim takes time and mental energy. Under cognitive load or stress, we fall back on heuristics and old beliefs. Information overload, Faced with vast amounts of conflicting information online, it’s easier to latch onto a simple narrative—even if it’s false. Halo effect. You form a strong positive (or negative) impression of a person, group or brand based on one salient trait. In Bayesian terms, that gives you an overly precise prior $p(z)$ about anything they say or do. New evidence from that source is up‑weighted—prediction errors are down‑weighted—so you keep believing them even when they’re wrong. Cognitive dissonance, When new evidence $x$ conflicts sharply with a held belief (prior $μ_{\rm prior}$), it generates large prediction error. To minimize that error the brain can either update its belief or reinterpret/ignore the evidence. Often it “chooses” the latter, twisting the data to fit the prior rather than admit error. Strong priors (from repetition, emotion, expertise, halo) plus error‑minimizing drives (disfavoring belief change under dissonance) lock you into false ideas. Under stress, group polarization or fatigue, precision on priors skyrockets and the brain leans even harder on these biases, making belief revision very unlikely."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://yingkui.com/css/dark.css><link rel=stylesheet type=text/css href=https://yingkui.com/css/custom.css><script src=https://yingkui.com/js/main.js></script><link rel=stylesheet href=https://yingkui.com/css/katex.min.css><script defer src=https://yingkui.com/js/katex.min.js></script><script defer src=https://yingkui.com/js/mhchem.min.js></script><script defer src=https://yingkui.com/js/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://yingkui.com/><img src=/logo-ai.jpg alt="Yingkui Lin"></a></div><h1 class=site-title><a href=https://yingkui.com/>Yingkui Lin</a></h1><div class=site-description><p>A Curious Mind.</p><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Essays</a></li><li><a href=/pages>Pages</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>24</span>
<span class=rest>Jul 2025</span></div></div><div class=matter><h1 class=title>Bias Prone Machine</h1></div></div><div class=markdown><p>Human minds are prone to holding on to false arguments and wrong ideas for several interacting reasons:</p><ol><li><strong>Confirmation bias</strong>, We tend to seek out or give extra weight to information that fits our existing beliefs, and ignore or downplay anything that contradicts them.</li><li><strong>Availability heuristic</strong>, Events or arguments that are more vivid or recently encountered feel more plausible than they objectively are.</li><li><strong>Dunning–Kruger effect</strong>, People with limited knowledge often overestimate their own competence, so they cling to simplistic but incorrect views.</li><li><strong>Strong priors</strong>, In the Bayesian brain framework, your “prior” beliefs act as predictions. When these priors are very confident, any new sensory evidence (prediction error) is down‐weighted and you won’t update your belief much.</li><li><strong>Precision weighting</strong>, The brain assigns a precision (inverse variance) to both priors and sensory data. If you overestimate the precision of your priors, you’ll ignore disconfirming evidence and reinforce a false idea.</li><li><strong>Identity‑protective cognition</strong>, Beliefs that tie into your group identity (political, religious, cultural) feel threatening if challenged, so you resist changing them even in the face of facts.</li><li><strong>Authority and conformity</strong>, If a respected leader or majority holds a wrong idea, you may adopt it to fit in or out of deference, rather than examine it critically.</li><li><strong>Backfire effect</strong>, Attempts to correct a false belief can paradoxically reinforce it, because disconfirming evidence feels like a personal attack.</li><li><strong>Sunk‐cost fallacy</strong>, Once you’ve invested time, emotion or reputation in an idea, admitting you were wrong feels like losing that investment.</li><li><strong>Effort avoidance</strong>, Deeply evaluating every claim takes time and mental energy. Under cognitive load or stress, we fall back on heuristics and old beliefs.</li><li><strong>Information overload</strong>, Faced with vast amounts of conflicting information online, it’s easier to latch onto a simple narrative—even if it’s false.</li><li><strong>Halo effect</strong>. You form a strong positive (or negative) impression of a person, group or brand based on one salient trait. In Bayesian terms, that gives you an overly precise prior $p(z)$ about anything they say or do. New evidence from that source is up‑weighted—prediction errors are down‑weighted—so you keep believing them even when they’re wrong.</li><li><strong>Cognitive dissonance</strong>, When new evidence $x$ conflicts sharply with a held belief (prior $μ_{\rm prior}$), it generates large prediction error. To minimize that error the brain can either update its belief or reinterpret/ignore the evidence. Often it “chooses” the latter, twisting the data to fit the prior rather than admit error.</li><li>Strong priors (from repetition, emotion, expertise, halo) plus error‑minimizing drives (disfavoring belief change under dissonance) lock you into false ideas. Under stress, group polarization or fatigue, precision on priors skyrockets and the brain leans even harder on these biases, making belief revision very unlikely.</li></ol></div><div class=tags></div></div></div><div class="footer wrapper"><nav class=nav><div>2025 © Copyright Yingkui.com All Rights Reserved</div></nav></div></body></html>