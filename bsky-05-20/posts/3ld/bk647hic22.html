<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Yingkui: "我第一次听说 attention 的时候

我以为就是，当你发现某些事情，需要细致的分析或存储的时候

我就把注意力移到那里，然后细致的分析，投入更多的计算资源"</title><link rel=stylesheet href="../../assets/style.css"><meta property=og:title content="Yingkui (@yingkui.com)"><meta property=og:description content="[quoting self] 

我第一次听说 attention 的时候

我以为就是，当你发现某些事情，需要细致的分析或存储的时候

我就把注意力移到那里，然后细致的分析，投入更多的计算资源"></head><body><div class=Root><div class=Page><div class=PageHeader><a href="../../index.html" class=Link>Home</a><a href="../../timeline/posts/1.html" class=Link>Timeline</a><a href="../../search.html" class=Link>Search</a></div><div class=PermalinkPost><div class=PermalinkPost__header><div class=PermalinkPost__avatarContainer><img loading=lazy src="../../blobs/bafkreih/62wkfwoeqra62rwzq4wxb5ifhmkbuoyjnwwdilpdpn2obzuuofq" class=PermalinkPost__avatar></div><span class=PermalinkPost__nameContainer><bdi class=PermalinkPost__displayNameContainer><span class=PermalinkPost__displayName>Yingkui</span></bdi><span class=PermalinkPost__handle>@yingkui.com</span></span></div><div class=PermalinkPost__body>我第一次听说 attention 的时候

我以为就是，当你发现某些事情，需要细致的分析或存储的时候

我就把注意力移到那里，然后细致的分析，投入更多的计算资源</div><div class=Embed><a href="biqfye2k2n.html" class="EmbedPost Interactive"><div class=EmbedPost__header><div class=EmbedPost__avatarContainer><img loading=lazy src="../../blobs/bafkreih/62wkfwoeqra62rwzq4wxb5ifhmkbuoyjnwwdilpdpn2obzuuofq" class=EmbedPost__avatar></div><span class=EmbedPost__nameContainer><bdi class=EmbedPost__displayNameContainer><span class=EmbedPost__displayName>Yingkui</span></bdi></span><span aria-hidden=true class=EmbedPost__dot>·</span><span class=EmbedPost__date>Dec 14, 2024</span></div><div class=EmbedPost__body><div class=EmbedPost__text>8/n

不喜欢 attention这个词，其实我觉得大脑运作有 更加 需要attention这个词的地方

maybe we will use stage in the future</div></div></a></div><div class=PermalinkPost__footer><time datetime="2024-12-14T14:47:51.533Z" class=PermalinkPost__date>December 14, 2024 at 10:47 PM</time></div></div><hr><div class=ThreadPage__descendants><div class=ReplyTree><div class=ReplyPost><div class=ReplyPost__aside><div class=ReplyPost__avatarContainer><img loading=lazy src="../../blobs/bafkreih/62wkfwoeqra62rwzq4wxb5ifhmkbuoyjnwwdilpdpn2obzuuofq" class=ReplyPost__avatar></div></div><div class=ReplyPost__main><div class=ReplyPost__header><span class=ReplyPost__nameContainer><bdi class=ReplyPost__displayNameContainer><span class=ReplyPost__displayName>Yingkui</span></bdi></span><span aria-hidden=true class=ReplyPost__dot>·</span><a href="bnc7fnsc22.html" aria-label="December 14, 2024 at 11:43 PM" class=ReplyPost__datetime><time datetime="2024-12-14T15:43:50.326Z">Dec 14, 2024</time></a></div><div class=ReplyPost__body>也就是说，如果需要attention，也就是动力系统需要我们有必要记住这个信息，有利于需求的满足，那么，就记住他更多抽象层级的信息，否则就记住比较少的，抽象的关键特征

但 transformer 里的 attention，其实是上下文中的相互影响（注意）</div></div></div></div></div></div></div></body></html>