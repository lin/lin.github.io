<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Yingkui: "Attention is all you have."</title><link rel=stylesheet href="../../assets/style.css"><meta property=og:title content="Yingkui (@yingkui.com)"><meta property=og:description content="[quoting self] 

Attention is all you have."></head><body><div class=Root><div class=Page><div class=PageHeader><a href="../../index.html" class=Link>Home</a><a href="../../timeline/posts/1.html" class=Link>Timeline</a><a href="../../search.html" class=Link>Search</a></div><div class=PermalinkPost><div class=PermalinkPost__header><div class=PermalinkPost__avatarContainer><img loading=lazy src="../../blobs/bafkreih/62wkfwoeqra62rwzq4wxb5ifhmkbuoyjnwwdilpdpn2obzuuofq" class=PermalinkPost__avatar></div><span class=PermalinkPost__nameContainer><bdi class=PermalinkPost__displayNameContainer><span class=PermalinkPost__displayName>Yingkui</span></bdi><span class=PermalinkPost__handle>@yingkui.com</span></span></div><div class=PermalinkPost__body>Attention is all you have.</div><div class=Embed><a href="bnc7fnsc22.html" class="EmbedPost Interactive"><div class=EmbedPost__header><div class=EmbedPost__avatarContainer><img loading=lazy src="../../blobs/bafkreih/62wkfwoeqra62rwzq4wxb5ifhmkbuoyjnwwdilpdpn2obzuuofq" class=EmbedPost__avatar></div><span class=EmbedPost__nameContainer><bdi class=EmbedPost__displayNameContainer><span class=EmbedPost__displayName>Yingkui</span></bdi></span><span aria-hidden=true class=EmbedPost__dot>·</span><span class=EmbedPost__date>Dec 14, 2024</span></div><div class=EmbedPost__body><div class=EmbedPost__text>也就是说，如果需要attention，也就是动力系统需要我们有必要记住这个信息，有利于需求的满足，那么，就记住他更多抽象层级的信息，否则就记住比较少的，抽象的关键特征

但 transformer 里的 attention，其实是上下文中的相互影响（注意）</div></div></a></div><div class=PermalinkPost__footer><time datetime="2024-12-14T18:10:42.044Z" class=PermalinkPost__date>December 15, 2024 at 2:10 AM</time></div></div><hr><div class=ThreadPage__descendants></div></div></div></body></html>