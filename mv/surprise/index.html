<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>- Yingkui Lin</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Yingkui Lin"><meta itemprop=description content="新词 FEP says: any system (like a brain) must minimize surprise (prediction error) by updating its internal model of the world.
To do that, it builds latent representations (hidden states) that explain incoming sensations and signals.
In humans, these latent states are often expressed through concepts and words
A new lexeme in science/economics (say “inflation”) is not just a word, but a chunked model of patterns in the world:
Before: the student sees prices rising, wages shifting, government printing money, but all feels disconnected → high surprise, fragmented predictions."><meta itemprop=wordCount content="812"><meta property="og:url" content="https://yingkui.com/mv/surprise/"><meta property="og:site_name" content="Yingkui Lin"><meta property="og:title" content="Yingkui Lin"><meta property="og:description" content="新词 FEP says: any system (like a brain) must minimize surprise (prediction error) by updating its internal model of the world.
To do that, it builds latent representations (hidden states) that explain incoming sensations and signals.
In humans, these latent states are often expressed through concepts and words
A new lexeme in science/economics (say “inflation”) is not just a word, but a chunked model of patterns in the world:
Before: the student sees prices rising, wages shifting, government printing money, but all feels disconnected → high surprise, fragmented predictions."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="mv"><meta name=twitter:card content="summary"><meta name=twitter:title content="Yingkui Lin"><meta name=twitter:description content="新词 FEP says: any system (like a brain) must minimize surprise (prediction error) by updating its internal model of the world.
To do that, it builds latent representations (hidden states) that explain incoming sensations and signals.
In humans, these latent states are often expressed through concepts and words
A new lexeme in science/economics (say “inflation”) is not just a word, but a chunked model of patterns in the world:
Before: the student sees prices rising, wages shifting, government printing money, but all feels disconnected → high surprise, fragmented predictions."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://yingkui.com/css/dark.css><link rel=stylesheet type=text/css href=https://yingkui.com/css/custom.css><script src=https://yingkui.com/js/main.js></script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://yingkui.com/><img src=/logo-ai.jpg alt="Yingkui Lin"></a></div><h1 class=site-title><a href=https://yingkui.com/>Yingkui Lin</a></h1><div class=site-description><p>A Curious Mind.</p><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Essays</a></li><li><a href=/micro>Microblogs</a></li><li><a href=/pages>Pages</a></li></ul></nav></div><div class=post><div class=post-header><div class=matter><h1 class=title></h1></div></div><div class=markdown><h3 id=新词>新词</h3><p>FEP says: any system (like a brain) must minimize surprise (prediction error) by updating its internal model of the world.</p><p>To do that, it builds latent representations (hidden states) that explain incoming sensations and signals.</p><p>In humans, these latent states are often expressed through concepts and words</p><p>A new lexeme in science/economics (say “inflation”) is not just a word, but a chunked model of patterns in the world:</p><p>Before: the student sees prices rising, wages shifting, government printing money, but all feels disconnected → high surprise, fragmented predictions.</p><p>After: with the lexeme “inflation,” the student compresses these events into one causal, generative explanation → surprise is reduced.</p><p>So a lexeme acts like a low-energy boundary condition (a Markov blanket for meaning).</p><p>Benefits in Terms of FEP</p><p>Lower Prediction Error</p><p>Once you learn “inflation,” your brain can predict price movements, news, policies with less confusion.</p><p>This reduces the free energy associated with interpreting economics signals.</p><p>Improved Generative Model</p><p>A lexeme is like adding a new latent variable into your internal Bayesian model.</p><p>This allows you to simulate and forecast future states more accurately.</p><p>Compression / Efficiency</p><p>Instead of storing 100 separate facts, you now bundle them under one compressed representation.</p><p>This saves cognitive resources, freeing up capacity for higher-level reasoning.</p><p>Action Guidance</p><p>With the concept of “inflation,” you know not to keep cash idle during high inflation.</p><p>Lexemes reduce uncertainty in deciding what action minimizes expected surprise.</p><p>Learning a lexeme is like to write a funciton in your brain, you just call the function “inflation()” → and get a quick, structured response.</p><p>Learning a new lexeme under the Free Energy Principle gives you a compressed, generative model that reduces prediction error, increases efficiency, and helps you act adaptively.</p><p>Latent variable in Bayesian terms = something hidden that explains your observations.</p><p>Lexeme = a compressed representation of such a hidden structure, which you can use to make sense of otherwise messy signals.</p><ol><li>If Learned in School (structured)</li></ol><p>The teacher gives you the lexeme (e.g., “inflation”), a clean definition, and examples.</p><p>This is like someone telling you directly: “here’s the latent variable that explains these observations.”</p><p>It saves you time and reduces trial-and-error learning.</p><p>You get to “install” the latent variable into your model quickly and align with the shared cultural/scientific framework.</p><p>If Not Learned in School (self-discovery)</p><p>You still encounter phenomena: prices rising, wages lagging, currency changes.</p><p>Without the label “inflation,” you struggle to unify them.</p><p>Eventually, you might invent your own internal lexeme, like “money rot” or “price sickness.”</p><p>Functionally, your brain is still searching for a hidden variable that reduces prediction error.</p><p>This is a more expensive route in terms of free energy minimization because:</p><p>You may take longer to notice the pattern.</p><p>Your invented concept may not match society’s shared lexeme, so communication costs rise.</p><p>You risk misgeneralizing (e.g., thinking “price sickness” is caused by merchants’ greed, not by macroeconomic forces).</p><p>Why Lexemes Are So Valuable</p><p>They are cultural priors: pre-compressed latent variables that others have already tested.</p><p>When you adopt them, you shortcut the painful process of discovering them on your own.</p><p>In Bayesian language: you’re inheriting a well-shaped prior distribution that reduces your personal free energy much faster.</p><p>Sometimes if you don’t get the lexeme from school (or other cultural sources), your brain may never actually converge on the right latent variable.</p><p>Why That Happens</p><p>Too much noise: The raw experiences are messy. Without the right compression, you can’t see the hidden structure.</p><p>Local explanations: The brain will “patch” things with folk explanations (e.g., “merchants are greedy” instead of “inflation is happening”).</p><p>Missed abstraction level: Some lexemes live at a very high abstraction (like “entropy” in physics). Without exposure, you may never invent it, because you’d need thousands of observations across domains.</p><p>Free Energy Perspective</p><p>Your generative model never gets upgraded with that latent variable.</p><p>That means your predictions remain noisy, error-prone, and costly.</p><p>You live in a world where surprise keeps accumulating, because you lack the compressed concept that would explain away the variation.</p><p>This is like running an algorithm without the right feature space: you keep overfitting to noise.</p><p>Real-World Examples</p><p>Economics:
Without “inflation” as a lexeme, a person might think “money just randomly loses value” or “shopkeepers are cheating.” They never see the systemic cause.</p><p>Biology:
Without the lexeme “germ theory,” people thought disease came from “bad air” or curses. Many never figured it out without schooling.</p><p>Physics:
Without “energy conservation,” phenomena like motion and heat stay mysterious. A craftsman may work with machines daily but never grasp why perpetual motion machines can’t exist.</p><p>Cultural Transmission as Energy Saver</p><p>That’s why education and culture are energy-saving machines.</p><p>They give you ready-made lexemes (latent variables) that humans couldn’t invent alone in a single lifetime.</p><p>Without them, you risk staying trapped in folk theories, never reaching the right model, no matter how much personal experience you have</p></div><div class=tags></div></div></div><div class="footer wrapper"><nav class=nav><div>2026 © Copyright Yingkui.com All Rights Reserved</div></nav></div></body></html>