<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Hopfield Networks Example - Yingkui Lin</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Hopfield Networks Example"><meta itemprop=description content="Generated from GPT-4o
It converges to a stable situation that is pre-defined.
Step-by-Step Example Letâ€™s say we want to store two 4-bit patterns:
$$ \text{Pattern A: } \xi^1 = [+1, -1, +1, -1] \newline \text{Pattern B: } \xi^2 = [-1, +1, -1, +1] $$Step 1: Compute the Weight Matrix Using the Hebbian learning rule:
$$ w_{ij} = \frac{1}{N} \sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu \quad \text{for } i \ne j $$Here, $N = 4$, $P = 2$"><meta itemprop=wordCount content="392"><meta property="og:url" content="https://yingkui.com/notes/hopfield/"><meta property="og:site_name" content="Yingkui Lin"><meta property="og:title" content="Hopfield Networks Example"><meta property="og:description" content="Generated from GPT-4o
It converges to a stable situation that is pre-defined.
Step-by-Step Example Letâ€™s say we want to store two 4-bit patterns:
$$ \text{Pattern A: } \xi^1 = [+1, -1, +1, -1] \newline \text{Pattern B: } \xi^2 = [-1, +1, -1, +1] $$Step 1: Compute the Weight Matrix Using the Hebbian learning rule:
$$ w_{ij} = \frac{1}{N} \sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu \quad \text{for } i \ne j $$Here, $N = 4$, $P = 2$"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hopfield Networks Example"><meta name=twitter:description content="Generated from GPT-4o
It converges to a stable situation that is pre-defined.
Step-by-Step Example Letâ€™s say we want to store two 4-bit patterns:
$$ \text{Pattern A: } \xi^1 = [+1, -1, +1, -1] \newline \text{Pattern B: } \xi^2 = [-1, +1, -1, +1] $$Step 1: Compute the Weight Matrix Using the Hebbian learning rule:
$$ w_{ij} = \frac{1}{N} \sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu \quad \text{for } i \ne j $$Here, $N = 4$, $P = 2$"><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://yingkui.com/css/dark.css><link rel=stylesheet type=text/css href=https://yingkui.com/css/custom.css><script src=https://yingkui.com/js/main.js></script><link rel=stylesheet href=https://yingkui.com/css/katex.min.css><script defer src=https://yingkui.com/js/katex.min.js></script><script defer src=https://yingkui.com/js/mhchem.min.js></script><script defer src=https://yingkui.com/js/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://yingkui.com/><img src=/logo-ai.jpg alt="Yingkui Lin"></a></div><h1 class=site-title><a href=https://yingkui.com/>Yingkui Lin</a></h1><div class=site-description><p>A Curious Mind.</p><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Essays</a></li><li><a href=/micro>Microblogs</a></li><li><a href=/pages>Pages</a></li></ul></nav></div><div class=post><div class=post-header><div class=matter><h1 class=title>Hopfield Networks Example</h1></div></div><div class=markdown><p><em>Generated from GPT-4o</em></p><p>It converges to a stable situation that is pre-defined.</p><h2 id=step-by-step-example>Step-by-Step Example</h2><p>Letâ€™s say we want to store <strong>two 4-bit patterns</strong>:</p>$$
\text{Pattern A: } \xi^1 = [+1, -1, +1, -1] \newline
\text{Pattern B: } \xi^2 = [-1, +1, -1, +1]
$$<h3 id=step-1-compute-the-weight-matrix>Step 1: Compute the Weight Matrix</h3><p>Using the Hebbian learning rule:</p>$$
w_{ij} = \frac{1}{N} \sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu \quad \text{for } i \ne j
$$<p>Here, $N = 4$, $P = 2$</p><p>We ignore diagonal elements $w_{ii} = 0$</p><p>We compute:</p>$$
W = \frac{1}{4} (\xi^1 (\xi^1)^T + \xi^2 (\xi^2)^T)
$$<p>Letâ€™s compute $\xi^1 (\xi^1)^T$:</p>$$
\xi^1 (\xi^1)^T =
\begin{bmatrix}
+1 \newline
-1 \newline
+1 \newline
-1
\end{bmatrix}
\begin{bmatrix}
+1 & -1 & +1 & -1
\end{bmatrix}
\newline
= \begin{bmatrix}
+1 & -1 & +1 & -1 \newline
-1 & +1 & -1 & +1 \newline
+1 & -1 & +1 & -1 \newline
-1 & +1 & -1 & +1 \newline
\end{bmatrix}
$$<p>Same for $\xi^2 (\xi^2)^T$, and then we average.</p><p>Final result:</p>$$
W =
\frac{1}{4} \left(
\begin{bmatrix}
0 & -2 & 2 & -2 \newline
-2 & 0 & -2 & 2 \newline
2 & -2 & 0 & -2 \newline
-2 & 2 & -2 & 0 \newline
\end{bmatrix}
\right)
$$$$
= \begin{bmatrix}
0 & -0.5 & 0.5 & -0.5 \newline
-0.5 & 0 & -0.5 & 0.5 \newline
0.5 & -0.5 & 0 & -0.5 \newline
-0.5 & 0.5 & -0.5 & 0 \newline
\end{bmatrix}
$$<h3 id=-step-2-recall-from-a-noisy-input>ðŸ§ª Step 2: Recall from a noisy input</h3><p>Letâ€™s input a <strong>noisy version of Pattern A</strong>:</p>$$
x = [+1, -1, -1, -1] \quad \text{(flipped one bit)}
$$<p>We update each unit $i$ by:</p>$$
x_i^{\text{new}} = \text{sign}\left(\sum_{j \ne i} w_{ij} x_j \right)
$$<p>Do this <strong>iteratively</strong> until convergence.</p><h4 id=example-update>Example: Update $x_3$</h4>$$
x_3^{\text{new}} = \text{sign}(w_{31} x_1 + w_{32} x_2 + w_{34} x_4) \newline
= \text{sign}(0.5 \cdot 1 + (-0.5) \cdot (-1) + (-0.5) \cdot (-1)) \newline
= \text{sign}(0.5 + 0.5 + 0.5) = \text{sign}(1.5) = +1
$$<p>Eventually, the network converges to:</p>$$
x = [+1, -1, +1, -1] = \xi^1
$$<p>ðŸŽ‰ <strong>It correctly recalled Pattern A!</strong></p><hr><h2 id=-what-this-reveals>ðŸ§  What this reveals</h2><ul><li>The <strong>network stores memories in its weight matrix</strong>.</li><li>It can <strong>recover full memories</strong> from <strong>partial or corrupted inputs</strong>.</li><li>Each memory is an <strong>attractor</strong> in the energy landscape.</li><li>This is an example of <strong>associative memory</strong>, showing how the brain might complete patterns from fragments.</li></ul></div><div class=tags></div></div></div><div class="footer wrapper"><nav class=nav><div>2026 Â© Copyright Yingkui.com All Rights Reserved</div></nav></div></body></html>