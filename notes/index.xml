<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Yingkui Lin</title><link>https://yingkui.com/notes/</link><description>Recent content on Yingkui Lin</description><generator>Hugo</generator><language>en-us</language><copyright>© Copyright Yingkui.com All Rights Reserved</copyright><lastBuildDate>Mon, 16 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://yingkui.com/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear Algebra Notes</title><link>https://yingkui.com/notes/la/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/la/</guid><description>&lt;h2 id="vector">Vector&lt;/h2>
&lt;p>Each vector is an object with many features values.&lt;/p>
&lt;h2 id="matrix">Matrix&lt;/h2>
&lt;p>Rows are an object with many features, Alice&amp;rsquo;s age gender height weight. Columns are Features for many objects, age of Alice Bob Chris David.&lt;/p>
&lt;p>$$
A = \begin{bmatrix}
\vec{c}_1 &amp;amp; \vec{c}_2 &amp;amp; \cdots &amp;amp; \vec{c}_n
\end{bmatrix}
$$&lt;/p>
&lt;p>$\vec{c}_1, \vec{c}_2, \cdots, \vec{c}_n$ are the base vectors after transformation described in current axis.&lt;/p>
&lt;p>Apply matrix to a vector $\vec{v}$ means you get what $\vec{v}$ means in current axis.&lt;/p></description></item><item><title/><link>https://yingkui.com/notes/2022-03-21/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/2022-03-21/</guid><description>&lt;h2 id="2022-03-21">2022-03-21&lt;/h2>
&lt;ol>
&lt;li>&lt;code>kex&lt;/code> means &lt;code>Key Exchange&lt;/code>&lt;/li>
&lt;/ol></description></item><item><title/><link>https://yingkui.com/notes/2023-12-22/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/2023-12-22/</guid><description>&lt;h3 id="midjourney">MidJourney&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>U means upscale / separation&lt;/p>
&lt;/li>
&lt;li>
&lt;p>V means variation&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>/imagine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>/blend&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="advanced">Advanced&lt;/h3>
&lt;ol>
&lt;li>image url&lt;/li>
&lt;li>text prompt&lt;/li>
&lt;li>params&lt;/li>
&lt;/ol>
&lt;h3>&lt;/h3>
&lt;ol>
&lt;li>Subject: person, animal, character, location, object, etc.&lt;/li>
&lt;li>Medium: photo, painting, illustration, sculpture, doodle, tapestry, etc.&lt;/li>
&lt;li>Environment: indoors, outdoors, on the moon, in Narnia, underwater, the Emerald City, etc.&lt;/li>
&lt;li>Lighting: soft, ambient, overcast, neon, studio lights, etc&lt;/li>
&lt;li>Color: vibrant, muted, bright, monochromatic, colorful, black and white, pastel, etc.&lt;/li>
&lt;li>Mood: Sedate, calm, raucous, energetic, etc.&lt;/li>
&lt;li>Composition: Portrait, headshot, closeup, birds-eye view, etc.&lt;/li>
&lt;/ol></description></item><item><title/><link>https://yingkui.com/notes/mechanics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/mechanics/</guid><description>&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/MIBfKJHMWHU?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
 &lt;/div>

&lt;h2 id="newtonian">Newtonian&lt;/h2>
&lt;p>$$
\vec{F} = \frac{\text d\vec{p}}{\text dt}
$$&lt;/p>
&lt;h2 id="lagrangian">Lagrangian&lt;/h2>
&lt;p>$$
L(q_i, \dot q_i, t) \;=\; T(q_i, \dot q_i) \;-\; V(q_i)
$$&lt;/p>
&lt;p>Kinetic energy minus the potential energy. And Euler–Lagrange equation:&lt;/p>
&lt;p>$$
\frac{d}{dt}\!\left(\frac{\partial L}{\partial \dot q_i}\right)
- \frac{\partial L}{\partial q_i}
= 0
$$&lt;/p>
&lt;h2 id="hamiltionian">Hamiltionian&lt;/h2>
&lt;p>$$
p_i \;=\; \frac{\partial L}{\partial \dot q_i}
$$&lt;/p></description></item><item><title>Free Energy Principle</title><link>https://yingkui.com/notes/fep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/fep/</guid><description>&lt;h2 id="the-free-energy-principle">The Free Energy Principle&lt;/h2>
&lt;p>$$
a, \mu, m = \arg\min F(\tilde{s}, \mu \mid m)
$$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ol>
&lt;li>$F(\tilde{s}, \mu \mid m)$ is the variational free energy, a quantity that bounds surprise.&lt;/li>
&lt;li>$\tilde{s}$: sensory inputs (possibly generalized coordinates of sensations).&lt;/li>
&lt;li>$\mu$: internal states (like beliefs or expectations).&lt;/li>
&lt;li>$m$: the model or structure used to generate predictions.&lt;/li>
&lt;li>$a$: actions that can influence the sensory input.&lt;/li>
&lt;li>$\arg\min$: denotes the values of $a, \mu, m$ that minimize the free energy.&lt;/li>
&lt;/ol>
&lt;p>This means that an agent (e.g. a brain) is constantly trying to:&lt;/p></description></item><item><title>Hopfield Networks Example</title><link>https://yingkui.com/notes/hopfield/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yingkui.com/notes/hopfield/</guid><description>&lt;p>&lt;em>Generated from GPT-4o&lt;/em>&lt;/p>
&lt;p>It converges to a stable situation that is pre-defined.&lt;/p>
&lt;h2 id="step-by-step-example">Step-by-Step Example&lt;/h2>
&lt;p>Let’s say we want to store &lt;strong>two 4-bit patterns&lt;/strong>:&lt;/p>
&lt;p>$$
\text{Pattern A: } \xi^1 = [+1, -1, +1, -1] \newline
\text{Pattern B: } \xi^2 = [-1, +1, -1, +1]
$$&lt;/p>
&lt;h3 id="step-1-compute-the-weight-matrix">Step 1: Compute the Weight Matrix&lt;/h3>
&lt;p>Using the Hebbian learning rule:&lt;/p>
&lt;p>$$
w_{ij} = \frac{1}{N} \sum_{\mu=1}^P \xi_i^\mu \xi_j^\mu \quad \text{for } i \ne j
$$&lt;/p>
&lt;p>Here, $N = 4$, $P = 2$&lt;/p></description></item></channel></rss>