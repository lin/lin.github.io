<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Machine Learning Terms - Yingkui Lin</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Machine Learning Terms"><meta itemprop=description content="$f(x; \theta)$ Model’s Prediction The function $f(x; \theta)$ represents the model’s output given input $x$ and parameters $\theta$. It defines how the model maps an input to a prediction.
$x$ is the input $\theta$ is the model’s parameters Example:
If $f(x; \theta) = \theta_1 x + \theta_0$, with $\theta_1 = 2$, $\theta_0 = 1$, and $x = 3$, then:
$f(3; \theta) = 2 \cdot 3 + 1 = 7$
$x$ Input Features $x$ is the input to the model. It can be a scalar, vector, or tensor depending on the task."><meta itemprop=datePublished content="2025-07-20T00:00:00+00:00"><meta itemprop=dateModified content="2025-07-20T00:00:00+00:00"><meta itemprop=wordCount content="1111"><meta property="og:url" content="https://yingkui.com/notes/ml/"><meta property="og:site_name" content="Yingkui Lin"><meta property="og:title" content="Machine Learning Terms"><meta property="og:description" content="$f(x; \theta)$ Model’s Prediction The function $f(x; \theta)$ represents the model’s output given input $x$ and parameters $\theta$. It defines how the model maps an input to a prediction.
$x$ is the input $\theta$ is the model’s parameters Example:
If $f(x; \theta) = \theta_1 x + \theta_0$, with $\theta_1 = 2$, $\theta_0 = 1$, and $x = 3$, then:
$f(3; \theta) = 2 \cdot 3 + 1 = 7$
$x$ Input Features $x$ is the input to the model. It can be a scalar, vector, or tensor depending on the task."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-07-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-20T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning Terms"><meta name=twitter:description content="$f(x; \theta)$ Model’s Prediction The function $f(x; \theta)$ represents the model’s output given input $x$ and parameters $\theta$. It defines how the model maps an input to a prediction.
$x$ is the input $\theta$ is the model’s parameters Example:
If $f(x; \theta) = \theta_1 x + \theta_0$, with $\theta_1 = 2$, $\theta_0 = 1$, and $x = 3$, then:
$f(3; \theta) = 2 \cdot 3 + 1 = 7$
$x$ Input Features $x$ is the input to the model. It can be a scalar, vector, or tensor depending on the task."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://yingkui.com/css/dark.css><link rel=stylesheet type=text/css href=https://yingkui.com/css/custom.css><script src=https://yingkui.com/js/main.js></script><link rel=stylesheet href=https://yingkui.com/css/katex.min.css><script defer src=https://yingkui.com/js/katex.min.js></script><script defer src=https://yingkui.com/js/mhchem.min.js></script><script defer src=https://yingkui.com/js/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://yingkui.com/><img src=/logo-ai.jpg alt="Yingkui Lin"></a></div><h1 class=site-title><a href=https://yingkui.com/>Yingkui Lin</a></h1><div class=site-description><p>A Curious Mind.</p><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Essays</a></li><li><a href=/pages>Pages</a></li><li><a href=/mv>MnV</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>20</span>
<span class=rest>Jul 2025</span></div></div><div class=matter><h1 class=title>Machine Learning Terms</h1></div></div><div class=markdown><h2 id=fx-theta>$f(x; \theta)$</h2><h3 id=models-prediction>Model&rsquo;s Prediction</h3><p>The function $f(x; \theta)$ represents the model&rsquo;s output given input $x$ and parameters $\theta$. It defines how the model maps an input to a prediction.</p><ul><li>$x$ is the input</li><li>$\theta$ is the model’s parameters</li></ul><p><strong>Example:</strong><br>If $f(x; \theta) = \theta_1 x + \theta_0$, with $\theta_1 = 2$, $\theta_0 = 1$, and $x = 3$, then:<br>$f(3; \theta) = 2 \cdot 3 + 1 = 7$</p><hr><h2 id=x>$x$</h2><h3 id=input-features>Input Features</h3><p>$x$ is the input to the model. It can be a scalar, vector, or tensor depending on the task.</p><p><strong>Example:</strong><br>$x = 4$ may represent 4 hours studied for a test.</p><hr><h2 id=theta>$\theta$</h2><h3 id=model-parameters>Model Parameters</h3><p>$\theta$ represents the parameters of the model, which are learned from data.</p><p><strong>Example:</strong><br>In a linear model $f(x) = \theta_1 x + \theta_0$, if $\theta_1 = 5$, $\theta_0 = -2$, then:<br>$f(2; \theta) = 5 \cdot 2 - 2 = 8$</p><hr><h2 id=haty>$\hat{y}$</h2><h3 id=predicted-output>Predicted Output</h3><p>$\hat{y} = f(x; \theta)$ is the model&rsquo;s predicted value for input $x$.</p><p><strong>Example:</strong><br>If $f(x; \theta) = 3x + 1$ and $x = 2$, then $\hat{y} = 7$</p><hr><h2 id=y>$y$</h2><h3 id=observed-true-output>Observed (True) Output</h3><p>$y$ is the actual label from the dataset.</p><p><strong>Example:</strong><br>If a student studied 2 hours and scored 9 points:<br>$x = 2$, $y = 9$</p><hr><h2 id=elly-haty>$\ell(y, \hat{y})$</h2><h3 id=per-sample-loss-function>Per-sample Loss Function</h3><p>$\ell(y, \hat{y})$ measures the difference between predicted output $\hat{y}$ and the true label $y$ for a single data point.</p><p><strong>Example:</strong><br>Using squared loss:<br>$\ell(y, \hat{y}) = (y - \hat{y})^2$<br>If $y = 9$, $\hat{y} = 7$, then:<br>$\ell = (9 - 7)^2 = 4$</p><hr><h2 id=mathcalltheta>$\mathcal{L}(\theta)$</h2><h3 id=empirical-risk--total-loss>Empirical Risk / Total Loss</h3><p>$\mathcal{L}(\theta)$ is the total loss across the dataset. It aggregates the per-sample losses:</p>$$
\mathcal{L}(\theta) \triangleq \frac{1}{N} \sum_{n=1}^{N} \ell(y_n, f(x_n; \theta))
$$<ul><li>$N$ is the number of training examples</li><li>$\ell(y_n, f(x_n; \theta))$ is the loss for the $n$-th example</li></ul><p><strong>Example:</strong><br>Let’s say $N=3$, and the model gives:</p><ul><li>$f(x_1) = 2$, $y_1 = 3$</li><li>$f(x_2) = 5$, $y_2 = 4$</li><li>$f(x_3) = 7$, $y_3 = 6$</li></ul><p>Then:</p>$$
\mathcal{L}(\theta) = \frac{1}{3}\left[(3-2)^2 + (4-5)^2 + (6-7)^2\right] = \frac{1}{3}(1 + 1 + 1) = 1
$$<hr><h2 id=py-mid-fx-theta>$p(y \mid f(x; \theta))$</h2><h3 id=likelihood-of-observed-label>Likelihood of Observed Label</h3><p>$p(y \mid f(x; \theta))$ gives the probability of observing label $y$ given the model’s prediction. Often used in probabilistic and Bayesian frameworks.</p><p><strong>Example:</strong><br>In logistic regression:</p><ul><li>$f(x; \theta) = \sigma(\theta^\top x)$</li><li>If $f(x; \theta) = 0.8$, then:</li></ul>$$
p(y = 1 \mid f(x; \theta)) = 0.8, \quad p(y = 0 \mid f(x; \theta)) = 0.2
$$<p>This means the model is 80% confident the true label is 1.</p><hr><h2 id=hattheta>$\hat{\theta}$</h2><h3 id=optimal-parameters>Optimal Parameters</h3><p>$\hat{\theta}$ is the parameter setting that minimizes the total loss $\mathcal{L}(\theta)$ on the training set. It represents the best-fit model under empirical risk minimization.</p><p>Formally:</p>$$
\hat{\theta} = \arg\min_{\theta} \mathcal{L}(\theta) = \arg\min_{\theta} \frac{1}{N} \sum_{n=1}^{N} \ell(y_n, f(x_n; \theta))
$$<p><strong>Example:</strong><br>If $\theta$ is a single weight and $\mathcal{L}(\theta)$ reaches its minimum at $\theta = 2.4$, then:<br>$\hat{\theta} = 2.4$</p><hr><h2 id=py-mid-x-theta>$p(y \mid x; \theta)$</h2><h3 id=predictive-distribution-probabilistic-view>Predictive Distribution (Probabilistic View)</h3><p>$p(y \mid x; \theta)$ represents the probability of label $y$ given input $x$ and model parameters $\theta$. It’s the <strong>predictive distribution</strong> used in probabilistic models, often derived from $f(x; \theta)$.</p><ul><li>In many models, $f(x; \theta)$ outputs parameters (like logits or means) used to compute this probability.</li></ul><p><strong>Example (Binary Classification):</strong><br>If $f(x; \theta) = 0.8$ is the probability that $y = 1$, then:</p>$$
p(y = 1 \mid x; \theta) = 0.8,\quad p(y = 0 \mid x; \theta) = 0.2
$$<p>This term is used extensively in maximum likelihood estimation and Bayesian inference frameworks.</p><p>Compare with $p(y \mid f(x; \theta))$, $p(y \mid x; \theta)$ is more general:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># p(y | x; θ)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>likelihood</span>(y, x, theta):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> full_distribution(y, x, theta)  <span style=color:#75715e># could be anything</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># p(y | f(x; θ))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>likelihood</span>(y, x, theta):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> f(x, theta)         <span style=color:#75715e># prediction or feature</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dist_y_given_z(y, z)  <span style=color:#75715e># e.g., softmax(z)</span>
</span></span></code></pre></div><hr><h2 id=nabla_theta-mathcalltheta>$\nabla_\theta \mathcal{L}(\theta)$</h2><h3 id=gradient-of-the-loss-function>Gradient of the Loss Function</h3><p>$\nabla_\theta \mathcal{L}(\theta)$ is the gradient of the total loss $\mathcal{L}$ with respect to the parameters $\theta$. It tells us how to adjust $\theta$ to reduce the loss.</p><p><strong>Example:</strong><br>If $\mathcal{L}(\theta) = (\theta - 2)^2$, then<br>$\nabla_\theta \mathcal{L}(\theta) = 2(\theta - 2)$<br>When $\theta = 3$, the gradient is $2(3 - 2) = 2$</p><hr><h2 id=hy>$H(Y)$</h2><h3 id=entropy>Entropy</h3><p>$H(Y)$ measures the uncertainty or randomness of a random variable $Y$. It quantifies how unpredictable $Y$ is.</p>$$
H(Y) = - \sum_{y} p(y) \log p(y)
$$<p><strong>Example:</strong><br>If $Y$ is a fair coin: $p(H) = p(T) = 0.5$, then<br>$H(Y) = -[0.5 \log 0.5 + 0.5 \log 0.5] = \log 2 \approx 0.693$</p><hr><h2 id=ix-y>$I(X; Y)$</h2><h3 id=mutual-information>Mutual Information</h3><p>$I(X; Y)$ measures how much information $X$ and $Y$ share. It quantifies the reduction in uncertainty of one variable given the other.</p>$$
I(X; Y) = H(Y) - H(Y \mid X)
$$<p><strong>Example:</strong><br>If $X$ and $Y$ are independent, $I(X; Y) = 0$<br>If $Y = X$, then knowing $X$ fully determines $Y$, and $I(X; Y) = H(Y)$</p><hr><h2 id=d_mathrmklp-vert-q>$D_{\mathrm{KL}}(P \Vert Q)$</h2><h3 id=kl-divergence-kullbackleibler>KL Divergence (Kullback–Leibler)</h3><p>$D_{\mathrm{KL}}(P \Vert Q)$ measures how one probability distribution $P$ diverges from another distribution $Q$. It is asymmetric and non-negative.</p>$$
D_{\mathrm{KL}}(P \Vert Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$<p><strong>Example:</strong><br>Let $P = [0.8, 0.2]$, $Q = [0.5, 0.5]$<br>Then<br>$D_{\mathrm{KL}}(P \Vert Q) = 0.8 \log \frac{0.8}{0.5} + 0.2 \log \frac{0.2}{0.5} \approx 0.257$</p><hr><h2 id=maximum-likelihood-estimation>Maximum Likelihood Estimation</h2><p>Suppose you have data $D = {x_1, x_2, &mldr;, x_n}$, and a model with parameter $\theta$. The <strong>likelihood function</strong> is:</p>$$
L(\theta) = P(D|\theta)
$$<p>MLE finds the $\theta$ that <strong>maximizes</strong> this:</p>$$
\hat{\theta}_{\text{MLE}} = \arg\max _{\theta} P(D|\theta)
$$<p>It just picks the parameter that makes the observed data most probable.</p><p>You flip a coin 10 times, and observe 7 heads and 3 tails.
Let $\theta$ be the probability of heads.</p><p>We want to find the value of $\theta$ that <strong>maximizes the likelihood</strong> of seeing exactly 7 heads.</p><p>The binomial likelihood for $k$ heads in $n$ trials is:</p>$$
P(k \text{ heads} \mid \theta) = \binom{n}{k} \cdot \theta^k \cdot (1 - \theta)^{n - k}
$$<p>In our case:</p>$$
L(\theta) = P(7 \text{ heads} \mid \theta) = \binom{10}{7} \cdot \theta^7 \cdot (1 - \theta)^3
$$<p>Let’s ignore the constant $\binom{10}{7}$, since it doesn’t depend on $\theta$. So we just maximize:</p>$$
L(\theta) \propto \theta^7 \cdot (1 - \theta)^3
$$<p>To maximize $\theta^7 (1 - \theta)^3$, we take the <strong>derivative</strong>, set it to zero, and solve.</p>$$
\ell(\theta) = \log L(\theta) = 7 \log \theta + 3 \log(1 - \theta)
$$<p>Take derivative:</p>$$
\frac{d\ell}{d\theta} = \frac{7}{\theta} - \frac{3}{1 - \theta}
$$<p>Set it to zero:</p>$$
\frac{7}{\theta} = \frac{3}{1 - \theta}
$$<p>Multiply both sides:</p>$$
7(1 - \theta) = 3\theta
\Rightarrow 7 - 7\theta = 3\theta
\Rightarrow 7 = 10\theta
\Rightarrow \theta = \frac{7}{10}
$$<p>The <strong>MLE estimate</strong> for $\theta$, the probability of heads, is:</p>$$
\boxed{\theta_{\text{MLE}} = \frac{7}{10}}
$$</div><div class=tags></div></div></div><div class="footer wrapper"><nav class=nav><div>2025 © Copyright Yingkui.com All Rights Reserved</div></nav></div></body></html>