<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>See A, Think B - Yingkui Lin</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="See A, Think B"><meta itemprop=description content="$P(B\mid A)$ The equation $E=mc^2$ may be the most celebrated in science, but its practical impact on our everyday lives is limited it dazzles more than it delivers. Even Feynman’s championing of atomic theory, though fundamentally important, doesn’t always translate into direct, universal benefits. In contrast, the principle of Bayesian inference expressed as
$$ P(B\mid A) =\frac{\text{count}(A\cap B)}{\text{count}(A)} $$
the probability of $B$ given that $A$ has occurred provides a versatile, widely applicable tool for updating beliefs and guiding decisions in countless real‑world situations."><meta itemprop=datePublished content="2025-07-15T00:00:00+00:00"><meta itemprop=dateModified content="2025-07-15T00:00:00+00:00"><meta itemprop=wordCount content="956"><meta property="og:url" content="https://yingkui.com/pba/"><meta property="og:site_name" content="Yingkui Lin"><meta property="og:title" content="See A, Think B"><meta property="og:description" content="$P(B\mid A)$ The equation $E=mc^2$ may be the most celebrated in science, but its practical impact on our everyday lives is limited it dazzles more than it delivers. Even Feynman’s championing of atomic theory, though fundamentally important, doesn’t always translate into direct, universal benefits. In contrast, the principle of Bayesian inference expressed as
$$ P(B\mid A) =\frac{\text{count}(A\cap B)}{\text{count}(A)} $$
the probability of $B$ given that $A$ has occurred provides a versatile, widely applicable tool for updating beliefs and guiding decisions in countless real‑world situations."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-15T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="See A, Think B"><meta name=twitter:description content="$P(B\mid A)$ The equation $E=mc^2$ may be the most celebrated in science, but its practical impact on our everyday lives is limited it dazzles more than it delivers. Even Feynman’s championing of atomic theory, though fundamentally important, doesn’t always translate into direct, universal benefits. In contrast, the principle of Bayesian inference expressed as
$$ P(B\mid A) =\frac{\text{count}(A\cap B)}{\text{count}(A)} $$
the probability of $B$ given that $A$ has occurred provides a versatile, widely applicable tool for updating beliefs and guiding decisions in countless real‑world situations."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://yingkui.com/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://yingkui.com/css/dark.css><link rel=stylesheet type=text/css href=https://yingkui.com/css/custom.css><script src=https://yingkui.com/js/main.js></script><link rel=stylesheet href=https://yingkui.com/css/katex.min.css><script defer src=https://yingkui.com/js/katex.min.js></script><script defer src=https://yingkui.com/js/mhchem.min.js></script><script defer src=https://yingkui.com/js/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://yingkui.com/><img src=/logo-ai.jpg alt="Yingkui Lin"></a></div><h1 class=site-title><a href=https://yingkui.com/>Yingkui Lin</a></h1><div class=site-description><p>A Curious Mind.</p><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Essays</a></li><li><a href=/pages>Pages</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>15</span>
<span class=rest>Jul 2025</span></div></div><div class=matter><h1 class=title>See A, Think B</h1></div></div><div class=markdown><h2 id=pbmid-a>$P(B\mid A)$</h2><p>The equation $E=mc^2$ may be the most celebrated in science, but its practical impact on our everyday lives is limited it dazzles more than it delivers. Even Feynman’s championing of atomic theory, though fundamentally important, doesn’t always translate into direct, universal benefits. In contrast, the principle of Bayesian inference expressed as</p><p>$$
P(B\mid A) =\frac{\text{count}(A\cap B)}{\text{count}(A)}
$$</p><p>the probability of $B$ given that $A$ has occurred provides a versatile, widely applicable tool for updating beliefs and guiding decisions in countless real‑world situations.</p><p>The deductive world of logic and computation only means all $P(B\mid A)$ is always near 1 or near 0.</p><h2 id=brain-counting-updates>Brain Counting Updates</h2><h2 id=bayes-theorem>Bayes’ Theorem</h2><p>Imagine a population divided into four groups:</p><ol><li>Male, Adult</li><li>Female, Adult</li><li>Male, Child</li><li>Female, Child</li></ol><p>We want to know: if a randomly chosen person is male, what is the probability that they are an adult?</p><ol><li><p>By definition,</p><p>$$
P(\text{Adult}\mid \text{Male})
= \frac{\text{number of Male Adults}}{\text{number of Males}}.
$$</p></li><li><p>Similarly,</p><p>$$
P(\text{Male}\mid \text{Adult})
= \frac{\text{number of Male Adults}}{\text{number of Adults}}.
$$</p></li><li><p>Since</p><p>$$
\text{number of Male Adults}
= P(\text{Male}\mid \text{Adult}) \times \text{number of Adults},
$$</p><p>we can write</p><p>$$
P(\text{Adult}\mid \text{Male})
= \frac{P(\text{Male}\mid \text{Adult})\;\times\;\text{number of Adults}}
{\text{number of Males}}.
$$</p></li><li><p>Dividing numerator and denominator by the total population size $N$, we get</p><p>$$
P(\text{Adult}\mid \text{Male})
= \frac{P(\text{Male}\mid \text{Adult})\;\times\;\dfrac{\text{number of Adults}}{N}}
{\dfrac{\text{number of Males}}{N}}
= \frac{P(\text{Male}\mid \text{Adult})\;P(\text{Adult})}{P(\text{Male})}.
$$</p></li></ol><p>This is <strong>Bayes’ theorem</strong> in its classic form:</p><p>$$
\boxed{
P(\mathrm{Adult}\mid \mathrm{Male})
= \frac{
\overset{\text{likelihood}}{P(\mathrm{Male}\mid \mathrm{Adult})}
\;\times\;
\overset{\text{prior}}{P(\mathrm{Adult})}
}{
\overset{\text{evidence}}{P(\mathrm{Male})}
}
}
$$</p><ul><li><strong>Prior</strong> $P(\text{Adult})$: your initial belief about how likely someone is an adult.</li><li><strong>Likelihood</strong> $P(\text{Male}\mid \text{Adult})$: how probable it is to observe “male” among adults.</li><li><strong>Evidence</strong> $P(\text{Male})$: the overall chance of picking a male.</li><li><strong>Posterior</strong> $P(\text{Adult}\mid \text{Male})$: your updated belief in “adult” once you know “male.”</li></ul><h3 id=why-it-matters>Why it matters</h3><p>Bayes’ rule tells you exactly how to <strong>update</strong> your prior belief in light of new evidence. In this example:</p><p>$$
\text{posterior} \;\longleftarrow\; \frac{\text{likelihood}\times\text{prior}}{\text{evidence}}.
$$</p><p>The more informative your likelihood, the more your posterior shifts away from the prior.</p><h2 id=class-level-observations>Class Level Observations</h2><blockquote><p>All men are mortal. Socrates is a man, therefore, Socrates is mortal.</p></blockquote><p>&ldquo;All men are mortal&rdquo; is a class property. &ldquo;Socrates&rdquo; is a class instance. &ldquo;Socrates is mortal&rdquo; means a class instance has to follow class properties.</p><p>Whenever a class A almost surely has property B in context H, any instance R of that class will also almost surely have property B in the same context.</p><p>$$
\bigl(R\subseteq A\;\wedge\;P(B\mid A,H)\ge1-\varepsilon\bigr)\;\Longrightarrow\;P(B\mid R,H)\ge1-\varepsilon.
$$</p><p>This is based on the assumption that additional information provided by R gives you no extra information about whether property B holds.</p><p>$$
P\bigl(B \mid A,\,H,\,R\bigr)
\;=\;
P\bigl(B \mid A,\,H\bigr)
$$</p><p>When we observe the reality, for each instance data we collect, we also gather the class level connections. When we think about instant-level priors, its class-level priors, which might be observed by a lot of times, should also be considered so that we are not losing too much the information gain by previous observations.</p><h3 id=physics-provides-strong-priors>Physics provides strong priors</h3><p>That is why a more knowledgable person can have generally better conclusions with high class-level priors.</p><h3 id=the-triumph-of-idiots>The Triumph of Idiots</h3><p>Class‑level observations can give us strong priors and thus high confidence but intuitively lumping data into broad categories doesn’t always lead to correct conclusions. Racism is a prime example: people reinforce the false belief that someone’s birthplace is the decisive cause of negative traits or outcomes.</p><h3 id=russells-turkey-is-doing-its-best>Russell’s turkey is doing its best</h3><p>When the turkey’s world has only two outcomes “fed” or “not fed” each morning—and it starts with a uniform Beta (1, 1) prior, the Bayesian predictive probability of breakfast tomorrow after $k$ feedings in $n$ days is</p><p>$$
P(\text{fed tomorrow}\mid k,n)=\frac{k+1}{n+2}.
$$</p><p>This Beta–binomial formula is the optimal forecast under those assumptions.
For instance, after 100 straight days of being fed ($k=n=100$) the turkey believes</p><p>$$
P(\text{fed tomorrow})=\frac{101}{102}\approx 0.99,
$$</p><p>i.e., a 99 % chance of one more meal.
Absent any other evidence no knowledge of Thanksgiving, butcher‑shop trucks, or class‑level information this $(k+1)/(n+2)$ rule is the best the turkey can do.</p><h3 id=connection-to-thepvalueand-the-weight-of-reality>Connection to the <em>p‑value</em> and the weight of reality</h3><p>A frequentist <em>p‑value</em> answers one narrowly defined question:</p><p>$$
p = P\bigl(\text{data at least this surprising}\mid H_0\bigr),
$$</p><p>the probability of seeing evidence as extreme as ours <strong>if</strong> the null hypothesis $H_0$ holds exactly.</p><p>When our current sample is tiny and virtually every prior observation in everyday life has agreed with $H_0$, this number can <strong>look</strong> like a Bayesian update in which $H_0$ carries an enormous prior weight: it merely flags that the new result would be rare under the long‑standing pattern.</p><p>But the practical question is different: Given all we already know about the world, is the alternative $H_1$ now more credible than $H_0$?</p><p>To answer that, a Bayesian compares <strong>posterior odds</strong></p><p>$$
\frac{P(H_1 \mid \text{data})}{P(H_0 \mid \text{data})}
=\;
\overset{\text{likelihood ratio}}
{\frac{P(\text{data}\mid H_1)}{P(\text{data}\mid H_0)}}
\;\times\;
\overset{\text{prior odds}}
{\frac{P(H_1)}{P(H_0)}}.
$$</p><ul><li><strong>Likelihood ratio</strong>: How well each hypothesis explains the <em>limited</em> data we have just collected.</li><li><strong>Prior odds</strong>: The vast “reality archive” of earlier observations, most of which have already favored $H_0$.</li></ul><p>A low p‑value signals that the new data are surprising under $H_0$; it says nothing about how plausible $H_1$ is once those towering prior odds are taken into account. Only by multiplying surprise (the likelihood ratio) by these priors do we learn whether our handful of fresh observations truly dents or merely ripples the immense body of evidence built up from everyday reality.</p><p>In short, <strong>p‑values measure surprise, not believability</strong>. Deciding whether $H_1$ “really works” demands folding that surprise into the giant pool of experience that has, so far, mostly vindicated $H_0$.</p><h3 id=roosters-crow-and-sunrise>Rooster&rsquo;s Crow and Sunrise</h3><p>$$
P\bigl(\text{sunrise}\mid \text{rooster crow},\,\text{dawn}\bigr) = 1
$$</p><p>$$
\forall t.\, P\bigl(\text{sunrise}\mid \text{rooster crow},\,t \bigr) = 1
$$</p><h2 id=features>Features</h2><h2 id=conceptual-leaps>Conceptual Leaps</h2></div><div class=tags></div></div></div><div class="footer wrapper"><nav class=nav><div>2025 © Copyright Yingkui.com All Rights Reserved</div></nav></div></body></html>