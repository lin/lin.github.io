<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Hell Fast: "也就是说，如果需要attention，也就是动力系统需要我们有必要记住这个信息，有利于需求的满足，那么，就记住他更多抽象层级的信息，否则就记住比较少的，抽象的关键特征

但 transformer 里的 attention，其实是上下文中的相互影响（注意）"</title><link rel=stylesheet href="../../assets/style.css"><meta property=og:title content="Hell Fast (@hellfast.bsky.social)"><meta property=og:description content="[replying to self] 

也就是说，如果需要attention，也就是动力系统需要我们有必要记住这个信息，有利于需求的满足，那么，就记住他更多抽象层级的信息，否则就记住比较少的，抽象的关键特征

但 transformer 里的 attention，其实是上下文中的相互影响（注意）"></head><body><div class=Root><div class=Page><div class=PageHeader><a href="../../index.html" class=Link>Home</a><a href="../../timeline/posts/1.html" class=Link>Timeline</a><a href="../../search.html" class=Link>Search</a></div><details class=ThreadAncestors><summary class="Interactive ThreadAncestors__header"><svg class=ThreadAncestors__accordionIcon viewBox="0 0 24 24"><path fill=currentColor d="M10 6L8.59 7.41L13.17 12l-4.58 4.59L10 18l6-6z"></path></svg><span class=ThreadAncestors__accordionText>Show parent replies</span></summary><div class=ThreadAncestors__list><div class=FeedPost><div class=FeedPost__context></div><div class=FeedPost__content><div class=FeedPost__aside><div class=FeedPost__avatarContainer><img loading=lazy src="../../blobs/bafkreid/u6lehy7vc2qy6ywgkhjazk2wa533g5tyxfeobi4jkxoyehkidui" class=FeedPost__avatar></div><div class=FeedPost__hasNextLine></div></div><div class=FeedPost__main><div class=FeedPost__header><span class=FeedPost__nameContainer><bdi class=FeedPost__displayNameContainer><span class=FeedPost__displayName>Hell Fast</span></bdi></span><span aria-hidden=true class=FeedPost__dot>·</span><a href="bk647hic22.html" aria-label="December 14, 2024 at 10:47 PM" class=FeedPost__date><time datetime="2024-12-14T14:47:51.533Z">Dec 14, 2024</time></a></div><div class=FeedPost__body>我第一次听说 attention 的时候

我以为就是，当你发现某些事情，需要细致的分析或存储的时候

我就把注意力移到那里，然后细致的分析，投入更多的计算资源</div><div class=Embed><a href="biqfye2k2n.html" class="EmbedPost Interactive"><div class=EmbedPost__header><div class=EmbedPost__avatarContainer><img loading=lazy src="../../blobs/bafkreid/u6lehy7vc2qy6ywgkhjazk2wa533g5tyxfeobi4jkxoyehkidui" class=EmbedPost__avatar></div><span class=EmbedPost__nameContainer><bdi class=EmbedPost__displayNameContainer><span class=EmbedPost__displayName>Hell Fast</span></bdi></span><span aria-hidden=true class=EmbedPost__dot>·</span><span class=EmbedPost__date>Dec 14, 2024</span></div><div class=EmbedPost__body><div class=EmbedPost__text>8/n

不喜欢 attention这个词，其实我觉得大脑运作有 更加 需要attention这个词的地方

maybe we will use stage in the future</div></div></a></div></div></div></div></div></details><div class=PermalinkPost><div class=PermalinkPost__header><div class=PermalinkPost__avatarContainer><img loading=lazy src="../../blobs/bafkreid/u6lehy7vc2qy6ywgkhjazk2wa533g5tyxfeobi4jkxoyehkidui" class=PermalinkPost__avatar></div><span class=PermalinkPost__nameContainer><bdi class=PermalinkPost__displayNameContainer><span class=PermalinkPost__displayName>Hell Fast</span></bdi><span class=PermalinkPost__handle>@hellfast.bsky.social</span></span></div><div class=PermalinkPost__body>也就是说，如果需要attention，也就是动力系统需要我们有必要记住这个信息，有利于需求的满足，那么，就记住他更多抽象层级的信息，否则就记住比较少的，抽象的关键特征

但 transformer 里的 attention，其实是上下文中的相互影响（注意）</div><div class=PermalinkPost__footer><time datetime="2024-12-14T15:43:50.326Z" class=PermalinkPost__date>December 14, 2024 at 11:43 PM</time></div></div><hr><div class=ThreadPage__descendants></div></div></div></body></html>